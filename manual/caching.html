<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Big Memory and Caching </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Big Memory and Caching ">
    <meta name="generator" content="docfx 2.12.1.0">
    
    <link rel="shortcut icon" href="../images/NFX.ico">
    <link rel="stylesheet" href="../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../styles/docfx.css">
    <link rel="stylesheet" href="../styles/main.css">
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
  </head>
  <body data-spy="scroll" data-target="#affix">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse nfx-navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              <a href="../index.html">
                <img id="nfx-logo" src="../images/NFX.Logo.Source.png" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="">
              <h1 id="big-memory-and-caching">Big Memory and Caching</h1>
              
<p>Managed runtimes can not handle more than 10 million objects without pauses for garbage collection. 
Sometimes even 10 MM resident objects start to pause process. GC Heap defragmentation kills the application by adding unpredictable stalls. 
The addition of physical RAM leads to increase of unpredictability even more as GC can now postpone its full sweep. 
So it is practically impossible to use in 64/128/256GB RAM nowadays, occupied by small CLR objects - the process just stalls.</p>
<p>All of the modern techniques (concurrent, background, parallel, server mode) try to overcome this situation but without any significant success. 
CLR objects consume lots of RAM, for example a string “ABC” holds around 30 bytes instead of 3. 
Java has the same issue, in spite of the fact that its VMs have way more options to control GC, but it is not enough for such big memory anyway. 
The root of the problem: the GC “sees” references and has to traverse them to see what is still reachable and what is not, obviously it takes time and slows down the application.</p>
<p>Many resident data objects are needed for different types of applications such as neural networks, caches, pre-computed data, etc. 
Stateless design is not applicable, actually it is not possible to keep all data in database because it is very slow and it is needed to cache data right in-process anyhow. 
It it is impossible to postpone garbage collection for a long time. </p>
<h2 id="pile">Pile</h2>
<p><strong>NFX</strong> solves the problem by  &quot;hiding&quot; data from GC.
Until recently there wasn’t any implementation of this concept. 
We did it in .NET. and called this technology Pile. 
Pile is a custom memory manager and it has 100%-managed code. 
Pile can store native CLR objects in a tightly-serialized form and free the managed garbage collector from having to deal with this objects. 
In addition piles can be either local (allocate local RAM on the server), or distributed (allocate RAM on many servers).
Contrary to many concerns of performance degradation due to serialization, Pile yields better results in terms of both time and space than using out-of-process serialization (i.e. Redis/Memcache).</p>
<p>The main ideas of Pile are</p>
<ul>
<li><p>To hold objects in huge byte arrays, so GC does not “see” them.</p>
</li>
<li><p>To use data teleportation through Slim Serializer (25% compression over CLR can be achieved).</p>
</li>
<li><p>To turn CLR object into struct{int,int} (segment and address within Pile) and back.</p>
</li>
<li><p>To manage “unmanaged” managed memory</p>
</li>
</ul>
<p>Pile allocates memory and returns PilePointer structure to the caller:</p>
<pre><code class="lang-csharp">public struct PilePointer : IEquatable&lt;PilePointer&gt;
{
  // Distributed Node ID. The local pile sets this to -1 rendering
  // this pointer as !DistributedValid
  public readonly int NodeID;

  // Segment # within pile
  public readonly int Segment;

  // Address within the segment (offset)
  public readonly int Address;
}
</code></pre><p>PilePointer represents a pointer to the pile object (object stored in a pile). 
The reference may be local or distributed in which case the NodeID is &gt;= 0.
Distributed pointers are very useful for organizing piles of objects distributed among many servers, for example  for &quot;Big Memory&quot; implementations or large neural networks where nodes may inter-connect between servers. 
The CLR reference to the IPile is not a part of this struct for performance and practicality reasons, as  it is highly unlikely that there are going to be more than one instance of a pile in a process, however should more than one pile be allocated than this pointer would need to be wrapped in some other structure along with source <code>IPile</code> reference.</p>
<p>With Pile it is possible to easily store 200,000,000 &quot;hot&quot; social connections with ability to traverse them in &lt;1ms by a single thread on a 64GB box. 
Therefore most of our applications turned from IO bound to CPU bound (pile teleportation) due to all required for application data are kept in RAM. 
Another benefit we get with using Pile is Cache which is described below. 
We can cache most of DB (except for financial rollups). Pile is very comfortable for Event Sourcing. 
We do not need to make any socket calls to Redis/Memcached because we already have everything that is needed  in memory, we don’t even need IPC/domain sockets. 
Pile-based applications tend to promote &quot;stateful&quot; paradigm and avoid talking to data layers at all, 95% requests are served from RAM. 
Important note:  we do not need to invent special Data Transfer Objects, as pile/cache stores Rows (along with logic) - this is classical OOP: data is here, it has methods and it has state. 
It allows making projects through Actor-based programming concept - Pile pointer becomes an Actor token. 
In spite of teleportation involved, Pile yields far faster throughput and better latency had we used plain CLR native objects that would have just stalled the process due to GC interruptions. 
In addition <code>NFX.ApplicationModel.Pile.Cache</code> that bases on Pile has expiration, priority, low/high watermarks.</p>
<p>We allocate a large byte array and then we sub-allocate chunks of that byte array and we hand it off to the user. 
So the user of the pile comes in and says that he wants to stick in the pile some object. 
This object can be any object, it does not have to be decorated, it can be tuple of strings or can be a dictionary of strings, list of strings and etc. 
Memory manager finds the space and it squeezes user’s object there but first it serializes the object to an array of bytes and then it stores that array of bytes in the pile and return to user pile pointer. 
When it is needed user can dereference the pile pointer back to the original object. 
Then memory manager takes the bytes from address within segment and deserializes it and give the original object back. 
It is can be thought of pile as a special kind of database that it is run within the process. 
Because of Slim serializer compresses the object (adaptive compression that uses variable encoding for integers and integer arrays and UTF8 strings, 
gets rid of machine word alignment padding) so that chunk within the segment is usually less in size (in bytes size) than the object in memory (every CLR object has header with flags for a walking, 
different GC flags and all of this kinda stuff and it stores information about fields). 
The cost of this compression is almost nonexistent because it is very fast.</p>
<p>When some object is deallocated from the pile corresponding block gets written back to the free buffer list. 
So every segment has a registry for the free buffers where it keeps the indexes of what is free. 
That does not induce any pressure on GC because everything happens internally. 
When it is needed to allocate some new object memory manager uses those free indexes to find the most appropriate slot. 
If it can find such slot it just returns it, occupies it and writes little header that the slot is taken. 
If there is no space in indices (it may depend on the situation with multi-threading contention/too many alloc/deallocs) everything go to the next segment and try to find space there. 
After it reaches some threshold it starts crawling the segment so that the crawl operation will visit all of the holes in the segment. 
So there is fragmentation possible but in the practical world this fragmentation is acceptable because you delete some information then you repopulate and you have some free chunks (similar to a hard drive). </p>
<p>In <strong>NFX</strong> the basis of Pile objects is <code>IPile</code> interface. 
Implementors of this interface are custom memory managers that favor the GC performance in apps with many objects at the cost of higher CPU usage. 
The implementor must be thread-safe for all operations unless stated otherwise on a member level. 
The memory represented by this class as a whole is not synchronizable, that is - it does not support functions like Interlocked-family, Lock, MemoryBarriers and the like that regular RAM supports. 
Should a need arise to interlock within the pile - a custom CLR-based lock must be used to synchronize access to pile as a whole, for example: a Get does not impose a lock on all concurrent writes through the pile (a write does not block all gets either). </p>
<p><code>IPile</code> interface has the following main methods:</p>
<ul>
<li><code>Put</code> puts a CLR object into the pile and returns a newly-allocated pointer. Throws out-of-space exception if there is not enough space in the pile and limits are set. Optional lifeSpanSec parameter will auto-delete object after the interval elapses if the pile supports data expiration.</li>
<li><code>Get</code> returns a CLR object by its pile pointer or throws access violation if pointer is invalid.</li>
<li><code>Delete</code> deletes object from pile by its pointer returning true if there is no access violation and pointer is pointing to the valid object, throws otherwise unless throwInvalid parameter is set to false (in many cases it is better to return bool result than throw slow exception). </li>
<li><code>Rejuvenate</code> if pile supports expiration, resets object age to zero, returns true if object was found and reset.</li>
<li><code>Purge</code> deletes all objects freeing all segment memory buffers. This method may require the caller to have special rights.</li>
<li><code>Compact</code> tries to delete extra capacity which is allocated but not currently needed.</li>
</ul>
<p>Implementation of this interface is <code>DefaultPile</code>. 
It has very extensive instrumentation like object count, segment count, allocated memory bytes, utilized bytes and all of that stuff. 
Also you can tune the pile to your particular case with putting your settings for the pile to configurations file.
Pile can be run on a web server, on a multi-threaded web-server with task parallel library with async/await. 
The Pile does not expose any asynchronous APIs and that is done on purpose because the pile usually works in not an IO- but in CPU-bound operations space, 
it does a lot of memory, a lot of CPU operations, and asynchronous APIs are not necessary.</p>
<p><strong>Example:</strong></p>
<pre><code class="lang-csharp">using (var pile = new DefaultPile())
{
  pile.Start();

  string str = &quot;Testing Pile&quot;;
  int i = 1002;

  var pp1 = pile.Put(str);
  var pp2 = pile.Put(i);

  var del = pile.Delete(pp1)      // true
  // var strNew = pile.Get(pp1);  - access violation
  var iNew = pile.Get(pp2);       // 1002
}
</code></pre><h2 id="caching">Caching</h2>
<p>While Pile provides a lower-level services of c-like <code>malloc()</code>, <code>PileCache</code> is build on top of Pile to efficiently store a key/value database in-process.
The cache consists of named tables each having a <code>TKey</code> generic argument.
This is important as we do not want to use boxing for keys as this would defeat the purpose of having Pile altogether.
Cache is speculative, that is: it does not guarantee that you get from if what you have written. 
It is a hashtable with a linear array buffer without relocating rehashing (it does do linear probing). 
The cache keeps a load factor (configurable) that keeps it at a steady hit rate. Entries have priorities which can preclude existing item eviction (i.e. a &quot;gold&quot; customer object may have a higher priority than a &quot;silver&quot; one).
Cache supports sliding and absolute expirations which are serviced by a dedicated sweep thread.
Both Cache and Pile come with a complete instrumentation suite (gauges).</p>

            </article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Copyright © 2006-2017 ITAdapter Corp Inc<br>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
